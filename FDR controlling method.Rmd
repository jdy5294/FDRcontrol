---
title: "FDR controling method"
author: "Jung Da Yeon"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. BH (Benjamini and Hochberg)

```{r BH, warning = FALSE, message=FALSE}
if (!require("stats")) install.packages("stats")
p = c(0.0001, 0.0004, 0.0019, 0.0095, 0.0201, 0.0278, 0.0298, 0.0344, 0.0459, 0.3240, 0.4262, 0.5719, 0.6528, 0.7590, 1.000)
BH = p.adjust(p, method = "BH", n = length(p))
print(data.frame(pvals = round(p,3), BH = round(BH, 3)))
print(sum(p <= 0.05, na.rm = TRUE))
print(sum(BH <= 0.05, na.rm = TRUE))
```

# 2. IHW (Independent Hypothesis Weighting)

```{r IHW, warning = FALSE, message=FALSE}
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

library(BiocManager)

# BiocManager::install("IHW")
# BiocManager::install("DESeq2")
# BiocManager::install("airway")

library("DESeq2")
library("dplyr")
library("IHW")

data("airway", package = "airway")
dds <- DESeqDataSet(se = airway, design = ~ cell + dex) %>% DESeq # ~ group + condition
deRes <- as.data.frame(results(dds))
ihwRes <- ihw(pvalue ~ baseMean,  data = deRes, alpha = 0.1) # 	formula, specified in the form pvalue~covariate

print(paste0('rejection # of ihw is ', rejections(ihwRes)))

print(paste0('rejection # of ihw is ', sum(adj_pvalues(ihwRes) <= 0.1, na.rm = TRUE), ', too'))

print(paste0('rejection # of original is ', sum(deRes$pvalue <= 0.1, na.rm = TRUE)))

print(paste0('rejection # of BH is ', sum(p.adjust(deRes$pvalue, method="BH", n = length(deRes$pvalue)) <= 0.1, na.rm = TRUE)))
```


# 3. Storey q-value

```{r qvalue, warning = FALSE, message=FALSE}
# BiocManager::install("qvalue",force = TRUE)

library(qvalue)

storey_qval <- qvalue(deRes$pvalue, fdr.level = 0.1)

print(paste0('rejection # of stoery q-value is ', sum(storey_qval$significant)))
```


# 4. BL (Boka and Leek)

avaliable in https://github.com/jgscott/FDRreg 

```{r FDRreg, warning = FALSE, message=FALSE, eval = FALSE, echo = FALSE}
FDRreg = function(z, features, nulltype='theoretical', method='pr', stderr = NULL, control=list()) {
# False discovery rate regression
# z = vector of z scores
# features = design matrix of covariates, assumed NOT to have an intercept just as in vanilla lm()
# nulltype = flag for what kind of null hypothesis to assume, theoretical/empirical/heteroscedastic
	
	stopifnot(any(method=='pr', method=='efron'))
	
	# Set up control parameters
	mycontrol = list(center=TRUE, scale=TRUE)
	if(method=='pr') {
		mycontrol$gridsize = 300
		mycontrol$decay = -0.67
		mycontrol$npasses = 10
		mycontrol$lambda = 0.01
		mycontrol$densknots=10
		mycontrol$nmids=150
	} else if(method=='efron') {
		mycontrol$gridsize = 150
		mycontrol$nmids=150
		mycontrol$densknots=10
	}
	# Overwrite with user choices
	mycontrol[(namc <- names(control))] <- control
	
	# Matrix of regressors, centered and scaled as requested
	N = length(z)
	X = scale(features, center=mycontrol$center, scale=mycontrol$scale)
	P = ncol(X)
	
	# Estimate the marginal density
	if(method=='pr') {
		
		# Compute M0 and M1, the marginals under null and alternative for each observation
		if(nulltype=='empirical') {

			# Currently using my implementation of Efron's central matching estimator
			l1 = efron(z, nmids=mycontrol$nmids, df=mycontrol$densknots, nulltype=nulltype)
			mu0 = l1$mu0
			sig0 = l1$sig0

			prfit = prfdr(z, mu0, sig0, control=mycontrol)
			fmix_grid = prfit$fmix_grid
			f0_grid = dnorm(prfit$x_grid, mu0, sig0)
			f1_grid = prfit$f1_grid
		} else if(nulltype=='heteroscedastic') {
			if(missing(stderr)) {
				stop("Must specify standard error (stderr) if assuming heteroscedastic null.")
			}
			mu0 = 0.0
			sig0 = stderr
			prfit = prfdr_het(z, mu0, sig0, control=mycontrol)
			fmix_grid = NULL
			f0_grid = NULL
			f1_grid = NULL
		} else {
			mu0 = 0.0
			sig0 = 1.0
			prfit = prfdr(z, mu0, sig0, control=mycontrol)
			fmix_grid = prfit$fmix_grid
			f0_grid = dnorm(prfit$x_grid, mu0, sig0)
			f1_grid = prfit$f1_grid
		}

		# Extract marginal densities and fit regression
		p0 = prfit$pi0
		M0 = dnorm(z, mu0, sig0)
		M1 = prfit$f1_z
		m1zeros = which(M1 < .Machine$double.eps)
		if(length(m1zeros > 0)) {
			M1[m1zeros] = min(M1[-m1zeros]) # substitute in the smallest nonzero value
			M1 = pmax(M1, .Machine$double.eps) # shouldn't happen but just in case!
		}
		m0zeros = which(M0 < .Machine$double.eps)
                if(length(m0zeros > 0)) {
                        M0[m0zeros] = min(M0[-m0zeros]) # substitute in the smallest nonzero value
                        M0 = pmax(M0, .Machine$double.eps) # shouldn't happen but just in case!
                }
		x_grid = prfit$x_grid
		regressfit = fdrr_regress_pr(M0, M1, X, 1-p0, lambda=mycontrol$lambda)

	} else if(method=='efron') {
		if(nulltype=='heteroscedastic') {
			stop("Cannot use Efron's method under a heteroscedastic null.")
		}
		l1 = efron(z, nmids=mycontrol$nmids, df=mycontrol$densknots, nulltype=nulltype)
		mu0 = l1$mu0
		sig0 = l1$sig0
		p0 = l1$p0
		M0 = dnorm(z, mu0, sig0)
		M1 = NULL
		MTot = l1$fz	
		x_grid = l1$mids
		fmix_grid = l1$zdens
		f0_grid = dnorm(x_grid, mu0, sig0)
		f1_grid = NULL
		regressfit = fdrr_regress_efron(M0, MTot, X, 1-p0, N)
	}

	out2 = getFDR(regressfit$PostProb)
	list(	z=z, X=X, localfdr=out2$localfdr, FDR=out2$FDR, x_grid = x_grid,
			M0 = M0, M1 = M1,
			fmix_grid=fmix_grid, f0_grid = f0_grid, f1_grid = f1_grid, 
			mu0=mu0, sig0=sig0, p0=p0, priorprob = regressfit$W,
			postprob = regressfit$PostProb, model=regressfit$model
    )

}



BayesFDRreg = function(z, features, mu0=NULL, sig0 = NULL, empiricalnull=FALSE, nmc=5000, nburn=1000,
	control=list(), ncomps=NULL, priorpars = NULL) {
# Fully Bayesian version of false discovery rate regression
# z = vector of z scores
# features = design matrix of covariates, assumed NOT to have an intercept just as in vanilla lm()
# nulltype = flag for what kind of null hypothesis to assume, theoretical or empirical
# ncomps = how many mixture components for the alternative hypothesis

	mycontrol = list(center=FALSE, scale=FALSE, verbose=nmc+nburn+1)
	mycontrol[(namc <- names(control))] <- control
	
	N = length(z)
	X = cbind(1,scale(features, center= mycontrol$center, scale= mycontrol$scale))
	P = ncol(X)
	
	if(empiricalnull) {
		l1 = efron(z, nmids=150, df=15, nulltype='empirical')
		mu0 = l1$mu0
		sig0 = l1$sig0
		p0 = l1$p0
	} else {
		if(missing(sig0)) sig0 = rep(1,N)
		if(missing(mu0)) mu0 = 0
		p0 = NULL
	}
	sig0squared = sig0^2
	M0 = dnorm(z, mu0, sig0)

	# Initialize MCMC
	if(missing(priorpars)) {
		PriorPrec = diag(rep(1/25, P))
		PriorMean = rep(0,P)
	} else{
		PriorPrec = priorpars$PriorPrec
		PriorMean = priorpars$PriorMean
	}
		
	if(missing(ncomps)) {
		foundfit = FALSE
		ncomps = 1
		emfit = deconvolveEM(z, ncomps)
		while(!foundfit) {
			newfit = deconvolveEM(z, ncomps+1)
			if(newfit$AIC > emfit$AIC) {
				foundfit = TRUE
			} else {
				emfit = newfit
				ncomps = ncomps+1
			}
		}
		M1 = dnormix(z, emfit$weights[1:ncomps]/sum(emfit$weights[1:ncomps]),
						emfit$means[1:ncomps], emfit$vars[1:ncomps])
	} else 	M1 = dnorm(z, 0, 4)
	
	PriorPrecXMean = PriorPrec %*% PriorMean
	Beta = rep(0,P)
	Beta[1] = -3
	BetaSave = matrix(0, nrow=nmc, ncol=P)
	MuSave = matrix(0, nrow=nmc, ncol=ncomps)
	VarSave = matrix(0, nrow=nmc, ncol=ncomps)
	WeightsSave = matrix(0, nrow=nmc, ncol=ncomps)
	PostProbSave = 0
	PriorProbSave = 0
	M1Save = 0
	
	# Alternative hypothesis
	comp_weights = rep(1/ncomps, ncomps)
	comp_means = quantile(z[abs(z/sig0)>2], probs=seq(0.025,0.975,length=ncomps))
	comp_variance = rep(1, ncomps)
	myvar = comp_variance + 1
	
	# Main MCMC
	for(t in 1:(nmc+nburn)) {
		
		if(t %% mycontrol$verbose == 0) cat(t, "\n")
		
		### Update indicators
		Psi = drop(X %*% Beta)
		W = ilogit(Psi)
		PostProb = W*M1/{(1-W)*M0 + W*M1}		
		Gamma = rbinom(N,1,PostProb)
		
		
		### Update mixture of normals model
		cases = which(Gamma==1)
		signals = z[cases]
		components = draw_mixture_component(signals, sig0[cases], weights=comp_weights, mu = comp_means, tau2 = comp_variance) + 1

		# Draw latent means
		if(length(cases) > 0) {
			latentmeans.var = 1.0/(1.0/sig0[cases]^2 + 1.0/comp_variance[components])
			latentmeans.mu = latentmeans.var*(signals/(sig0[cases]^2) + (comp_means/comp_variance)[components])
			latentmeans = rnorm(length(cases), latentmeans.mu, sqrt(latentmeans.var))
			nsig = mosaic::maggregate(signals ~ factor(components, levels=1:ncomps), FUN='length')
			tss_thetai = mosaic::maggregate((latentmeans-comp_means[components])^2 ~ factor(components, levels=1:ncomps), FUN='sum')
			sum_thetai = mosaic::maggregate(latentmeans ~ factor(components, levels=1:ncomps), FUN='sum')
		} else {
			nsig = rep(0,ncomps)
			tss_thetai = rep(0,ncomps)
			mean_thetai = rep(0,ncomps)
		}

		# Actual updates
		for(k in 1:ncomps) comp_variance[k] = 1.0/rgamma(1, {nsig[k]+2}/2, rate={tss_thetai[k]+2}/2)
		muvar = comp_variance/{nsig + comp_variance*0.1}
		muhat = sum_thetai/{nsig + comp_variance*0.1}
		comp_means = rnorm(ncomps, muhat, sqrt(muvar))	
		comp_weights = rdirichlet_once(rep(5, ncomps) + nsig)
		M1 = marnormix(z, sig0squared, comp_weights, comp_means, comp_variance)

		### Update latent variables in logit likelihood
		Om = as.numeric(BayesLogit::rpg(N,rep(1,N),Psi))

		### Update regression parameters
		Kap = PostProb - 0.5
		PrecMat = t(X) %*% {Om * X} + PriorPrec
		Beta.V = solve(PrecMat)
		Beta.mu = Beta.V %*% {t(X) %*% Kap + PriorPrecXMean}
		Beta = t(mvtnorm::rmvnorm(1,mean=Beta.mu,sigma=Beta.V))	
		if(t > nburn) {
			BetaSave[t-nburn,] = Beta
			MuSave[t-nburn,] = comp_means
			VarSave[t-nburn,] = comp_variance
			WeightsSave[t-nburn,] = comp_weights
			PostProbSave = PostProbSave + (1.0/nmc)*PostProb
			PriorProbSave = PriorProbSave + (1.0/nmc)*W
			M1Save = M1Save + (1.0/nmc)*M1
		}
	}
	out2 = getFDR(PostProbSave)
		
	mylist = list(z=z, localfdr=out2$localfdr, FDR=out2$FDR, X=X,
			M0 = M0, M1 = M1Save, mu0=mu0, sig0=sig0, p0=p0, ncomps=ncomps,
			priorprob = PriorProbSave, postprob = PostProbSave, 
			coefficients = BetaSave, weights = WeightsSave, means=MuSave, vars = VarSave
    )
	return(mylist);
}

```

avaliable in https://github.com/SiminaB/Fdr-regression

```{r, warning = FALSE, message=FALSE, eval = FALSE, echo = FALSE}
##logistic regression version of estimate!
lm_pi0 <- function(pValues, lambda = seq(0.05, 0.95, 0.05), X, smooth.df=3, threshold=TRUE)
{
  ##if X is a vector, change it into a matrix
  if(is.null(dim(X)))
  {
    X <- matrix(X, ncol=1)
  }
  
  ##number of tests
  n <- nrow(X)
  ##number of lambdas
  nLambda <- length(lambda)
  
  ##sort lambdas from smallest to largest and take only unique values
  lambda <- sort(unique(lambda))
  
  ##make a design matrix with the intercept
  Xint <- cbind(1, X)
  
  ##get the estimate for each value of lambda 
  pi0.lambda <- matrix(NA, nrow=n, ncol=nLambda)
  for(i in 1:nLambda)
  {
    lambda.i <- lambda[i]
    y <- pValues > lambda.i
    
    ##fit regression
    regFit <- glm(y ~ X, family=binomial)
    
    ##get the estimated values of pi0
    pi0.lambda[,i] <- regFit$fitted.values/(1-lambda.i)
    
    if(threshold){
      pi0.lambda[,i] <- ifelse(pi0.lambda[,i] > 1, 1, pi0.lambda[,i])
      pi0.lambda[,i] <- ifelse(pi0.lambda[,i] < 0, 0, pi0.lambda[,i])
    }
  }
  
  ##smooth over values of lambda (do this for each test in part)
  pi0.smooth <- matrix(NA, nrow=n, ncol=nLambda)
  ##also save final estimate (maximum of 0 and minimum of 1 and smoothed value at largest lambda)
  pi0 <- rep(NA, length=n)
  for(i in 1:n)
  {
    if(i %% 10000==0)
    {
      message(paste("At test #:",i))
    }
    spi0 <- smooth.spline(lambda, pi0.lambda[i,], df=smooth.df)
    pi0.smooth[i, ] <- spi0$y
    pi0[i] <- pi0.smooth[i,nLambda]
  }
  
  if(threshold){ 
    pi0 <- ifelse(pi0 > 1, 1, pi0)
    pi0 <- ifelse(pi0 < 0, 0, pi0)
  }
  
  return(list(pi0=pi0, pi0.lambda=pi0.lambda, lambda=lambda, pi0.smooth=pi0.smooth))
}



##------Functions of covariates-------##
f1 <- function(x){
  p2 <- -0.2
  p1 <- 1.2
  a <- 4/(p1-p2)^2
  
  y <- -a*(x-p1)*(x-p2)
  y[x >= 0.7] <- -a*(0.7-p1)*(0.7-p2)
  y[x <= (p1+p2)/2] <- 1  
  y
}

f2 <- function(x){
  y <- rep(0, length=length(x))
  y[x >= 0.7] <- -2.5*(x[x >= 0.7]-0.7)^2  
  y
}

f3 <- function(x){
  y <- rep(0, length=length(x))
  y[x < 0.7] <- -(x[x < 0.7]-0.1)^2
  y[x >= 0.7] <- -(min(x[x >= 0.7])-0.1)^2
  y[x<=0.1] <- 0
  y
}

##smooth function of one covariate for different levels of second covariate
f <- function(x1,x2){
  y1 <- f1(x1)
  y2 <- f2(x1)
  y3 <- f3(x1)
  
  y <- rep(0, length(x1))
  y[x2 == 1] <- y1[x2 == 1] + y2[x2 == 1] + 0.12*y3[x2 == 1]
  y[x2 == 2] <- y1[x2 == 2] + 0.5*y2[x2 == 2] + 0.06*y3[x2 == 2]
  y[x2 == 3] <- y1[x2 == 3] + 0.3*y2[x2 == 3] 
  
  y
}

##smooth function of a single variable
fSingle <- function(x){
  y1 <- f1(x)
  y2 <- f2(x)
  y3 <- f3(x)
  
  y <- rep(0, length(x))
  ##y <- y1 + 0.5*y2 + 0.06*y3
  
  y <- y1 + y2 + 0.12*y3
  
  y
}

##------Functions to generate independent p-values-------##
genPvalsIndNorm <- function(pi0, muAlt)
{
  ntest <- length(pi0)
  
  nullI <- rbinom(ntest,prob=pi0,size=1)> 0
  
  n1 <- floor(ntest/2)
  ##simulate means
  mu <- c(rnorm(n1,muAlt,1),rnorm(ntest-n1,-muAlt,1))

  mu[nullI == 1] <- 0
  
  zValues <- rnorm(ntest, mu, 1)
  
  pValues <- 2*(1-pnorm(abs(zValues)))
  
  list(zValues=zValues, pValues=pValues, null=nullI)
}

genPvalsIndT <- function(pi0, muAlt, n=6)
{
  ntest <- length(pi0)
  
  nullI <- rbinom(ntest,prob=pi0,size=1)> 0
  
  n1 <- floor(ntest/2)
  ##simulate means
  mu <- c(rnorm(n1,muAlt,1),rnorm(ntest-n1,-muAlt,1))
  
  mu[nullI == 1] <- 0
  
  tValues <- rep(NA, ntest)
  
  df.t <- 2*n-2
  ##get multiplication factor to get from ncp to mean
  mult <- sqrt(df.t/2)*gamma((df.t-1)/2)/gamma(df.t/2)
  for(i in 1:ntest)
  {
    tValues[i] <- rt(1, df=df.t, ncp=mu[i]/mult)
  }
  
  pValues <- 2*(1-pt(abs(tValues), df=df.t))

  list(zValues=tValues, pValues=pValues, null=nullI)
}

genPvalsIndChisq <- function(pi0, muAlt, r=2, c=2)
{
  ntest <- length(pi0)
  
  nullI <- rbinom(ntest,prob=pi0,size=1)> 0
  
  ##simulate non-centrality parameters
  ncp <- (rnorm(ntest,muAlt,1))^2
    
  ncp[nullI == 1] <- 0
  
  chisqValues <- rep(NA, ntest)
  
  df.chisq <- (r-1)*(c-1)
  for(i in 1:ntest)
  {
    chisqValues[i] <- rchisq(1, df=df.chisq, ncp=ncp[i])
  }
  
  pValues <- 1-pchisq(chisqValues, df=df.chisq)
  
  zValues <- qnorm(1-pValues/2)
  
  list(zValues=zValues, pValues=pValues, null=nullI)
}

genPvalsIndBeta <- function(pi0, shape2)
{
  ntest <- length(pi0)
  
  nullI <- rbinom(ntest,prob=pi0,size=1)> 0
  
  pValues <- rep(NA,ntest)
  pValues[nullI] <- runif(sum(nullI))
  pValues[!nullI] <- rbeta(sum(!nullI),1,shape2)
  
  zValues <- qnorm(1-pValues/2)
  
  list(zValues=zValues, pValues=pValues, null=nullI)
}

##------Functions to generate p-values from correlated normal or t distributions----##

genPvalsCorrNorm <- function(pi0, muAlt, Sigma)
{
  ntest <- length(pi0)
  
  nullI <- rbinom(ntest,prob=pi0,size=1)> 0
  
  n1 <- floor(ntest/2)
  ##simulate means
  mu <- c(rnorm(n1,muAlt,1),rnorm(ntest-n1,-muAlt,1))
  
  mu[nullI == 1] <- 0
  
  zValues <- rmnorm(1, mu, Sigma)
  
  pValues <- 2*(1-pnorm(abs(zValues)))
  
  list(zValues=zValues, pValues=pValues, null=nullI)
}

genPvalsCorrT <- function(pi0, muAlt, Sigma, n=6)
{
  ntest <- length(pi0)
  
  nullI <- rbinom(ntest,prob=pi0,size=1)> 0
  
  n1 <- floor(ntest/2)
  ##simulate means
  mu <- c(rnorm(n1,muAlt,1),rnorm(ntest-n1,-muAlt,1))
  
  mu[nullI == 1] <- 0
  
  tValues <- rep(NA, ntest)
  
  df.t <- 2*n-2

  tValues <- rmt(1, S=Sigma*(df.t-2)/df.t, df = df.t, mean = mu)
  
  pValues <- 2*(1-pt(abs(tValues), df=df.t))
  
  list(zValues=tValues, pValues=pValues, null=nullI)
}

##------Function to plot means and true values of pi0------##
plotMeanPi0 <- function(pi0, pi0Means, pi0ScottMean, pi0StoreyMean, tme, xi1=TRUE, main="I")
{
  par(cex.axis = 1.1, cex.main=1.3,
      mar=c(5.1, 4.1, 4.1, 14.6), xpd=TRUE)
  
  ##defined colors so that we have transparencies
  blackT <- rgb(0,0,0,alpha=0.3) 
  blueT <- rgb(0,0,1,alpha=0.3)
  orangeT <- rgb(1,0.65,0,alpha=0.3)
  
  pi0StoreyMean <- rep(pi0StoreyMean, length(pi0))
  
  plot(pi0 ~ tme,col="white",type="p",lwd=8, lty=1,
       xlab="", yaxt = "n",
       ylim=c(0.3,1), ylab="",
       main=main, pch=19, cex=0.1)
  points(pi0 ~ tme,pch=19, col=blackT, cex=0.3)
  if(xi1)
  {
    mtext(expression(x[i1]), 1, line=3, cex=1.3)
  } else {
    mtext(expression(x[i]), 1, line=3, cex=1.3)
  }
  mtext(expression(paste("Mean ", hat(pi)[0](x[i])," and ", pi[0](x[i]))), 2, line=2, cex=1.3)
  ##points(pi0Means$pi0hatMean0.8 ~ tme,col="brown",type="p",lwd=2, lty=3, pch=19, cex=0.2)
  ##points(pi0Means$pi0hatMean0.9 ~ tme,col="orange",type="p",lwd=2, lty=2, pch=3, cex=0.2)
  points(pi0ScottMean ~ tme,col=blueT,type="p",lwd=3, lty=1, pch=19, cex=0.2)
  points(pi0Means$pi0hatMeanFinal ~ tme,col=orangeT,type="p",lwd=3, lty=1, pch=19, cex=0.2)
  points(pi0StoreyMean ~ tme,col="brown",lwd=2, lty=1, pch=19, cex=0.2)

  axis(side=2, at=c(0.3,0.5,0.7,0.9), mgp=c(3, 0.7, 0)) 
}


##-------Functions for the variance plots-----------##

##get upper bound for variance
##z does not include the intercept (which gets added in the function)
getVarBound <- function(z, lambda)
{
  zMat <- cbind(1, z)
  S <- zMat%*%solve(t(zMat)%*%zMat)%*%t(zMat)
  diag(S)/(4*(1-lambda)^2)
}

##plot variance and upper bound
plotVarBound <- function(pi0hatVarBound, pi0hatVar, tme, xi1=TRUE)
{
  plot(pi0hatVarBound ~ tme, col="red", ylim=c(0, max(pi0hatVarBound)),
       lwd=3, lty=3,
       type="l", 
       xlab="", ylab="")
  if(xi1)
  {
    mtext(expression(x[i1]), 1, line=3, cex=1.3)
  } else {
    mtext(expression(x[i]), 1, line=3, cex=1.3)
  }
  mtext(expression(paste("Variance and upper bound of variance for ", " ", hat(pi)[0](x[i]), sep=" ")), 2, line=2, cex=1.3)
  points(pi0hatVar ~ tme, col="black", type="l", lwd=3, lty=1)
  legend("top", ##x=-0.4, y=0.2, 
         legend=c("Empirical variance", "Upper bound"),
         col=c("black", "red"), bty="n",
         lwd=c(3,3), lty=c(1,3),
         cex=1.2, x.intersp=0.2, y.intersp=1.0)  
}

##------Function to get fraction of false discoveries and fraction of true positives over some number of simulations------##
##helper function to get discoveries at a given threshold
discThresh <- function(q, alpha=0.05)
{
  d <- NULL
  if(length(q) > 0)
  {
   d <- q <= alpha 
  }
  d
}

##helper function to get fraction of false discoveries (i.e. they are discovered AND they are null) out of the number of all discoveries for each simulation
estFDR <- function(disc, nullHypSims)
{
  fdr <- rowSums(disc * nullHypSims)/rowSums(disc)
  fdr[is.na(fdr)] <- 0
  fdr
}
##helper function to get fraction of true positives (i.e. they are discovered AND they are not null) out of the number of all non-nulls for each simulation
estTPR <- function(disc, nullHypSims)
{
  tpr <- rowSums(disc * (1-nullHypSims))/rowSums(1-nullHypSims)
  tpr[is.na(tpr)] <- 0
  tpr
}

estFDR.TPR <- function(FDR.BL, FDR.BH, FDR.Storey, FDR.Scott=NULL, FDR.Scott_emp=NULL, nullHypSims)
{
  ##first get all the discoveries at 0.05:
  discBL <- discThresh(FDR.BL, 0.05)
  discBH <- discThresh(FDR.BH, 0.05)
  
  ##for Storey method, only use simulations that resulted in non-NA values
  discStorey <- NULL
  nullHypSims.Storey <- nullHypSims
  if(length(FDR.Storey) > 0)
  {
    which.nonNA.Storey <- which(!is.na(rowSums(FDR.Storey)))
    FDR.Storey <- FDR.Storey[which.nonNA.Storey,]
    nullHypSims.Storey <- nullHypSims[which.nonNA.Storey,]
    
    discStorey <- discThresh(FDR.Storey, 0.05)
  }  
  
  discScott <- discThresh(FDR.Scott, 0.05)

  ##for Scott empirical method, only use simulations that resulted in non-NA values
  discScott_emp <- NULL
  nullHypSims.Scott_emp <- nullHypSims
  if(length(FDR.Scott_emp) > 0)
  {
    which.nonNA.Scott_emp <- which(!is.na(rowSums(FDR.Scott_emp)))
    FDR.Scott_emp <- FDR.Scott_emp[which.nonNA.Scott_emp,]
    nullHypSims.Scott_emp <- nullHypSims[which.nonNA.Scott_emp,]
    
    discScott_emp <- discThresh(FDR.Scott_emp, 0.05)
  }
  
  ##now get fraction of false discoveries 
  fdrBL <- estFDR(discBL, nullHypSims)
  fdrBH <- estFDR(discBH, nullHypSims)

  fdrStorey <- rep(NA, length(discBL))
  if(length(discStorey) == length(nullHypSims.Storey))
  {
    fdrStorey <- estFDR(discStorey, nullHypSims.Storey)
  }
  fdrScott <- rep(NA, length(discBL))
  if(length(discScott) == length(nullHypSims))
  {
    fdrScott <- estFDR(discScott, nullHypSims)
  }
  fdrScott_emp <- rep(NA, length(discBL))
  if(length(discScott_emp) == length(nullHypSims.Scott_emp))
  {
    fdrScott_emp <- estFDR(discScott_emp, nullHypSims.Scott_emp)
  }

  ##also get fraction of true discoveries out of the number of all alternatives
  tprBL <- estTPR(discBL, nullHypSims)
  tprBH <- estTPR(discBH, nullHypSims)

  tprStorey <- rep(NA, length(discBL))
  if(length(discStorey) == length(nullHypSims.Storey))
  {
    tprStorey <- estTPR(discStorey, nullHypSims.Storey)
  }  
  tprScott <- rep(NA, length(discBL))
  if(length(discScott) == length(nullHypSims))
  {
    tprScott <- estTPR(discScott, nullHypSims)
  }
  tprScott_emp <- rep(NA, length(discBL))
  if(length(discScott_emp) == length(nullHypSims.Scott_emp))
  {
    tprScott_emp <- estTPR(discScott_emp, nullHypSims.Scott_emp)
  }
  
  FDR.TPR <- matrix(NA, nrow=5, ncol=3)
  colnames(FDR.TPR) <- c("FDR","TPR","Percent used")
  rownames(FDR.TPR) <- c("BL","Scott","Scott_emp","Storey","BH")
  
  FDR.TPR["BL",] <- c(mean(fdrBL), mean(tprBL), nrow(FDR.BL)/nrow(nullHypSims)*100)
  FDR.TPR["Scott",] <- c(mean(fdrScott), mean(tprScott), nrow(FDR.Scott)/nrow(nullHypSims)*100)
  if(length(dim(FDR.Scott_emp))==2)
  {
    FDR.TPR["Scott_emp",] <- c(mean(fdrScott_emp), mean(tprScott_emp), nrow(FDR.Scott_emp)/nrow(nullHypSims)*100)
  } else {
    FDR.TPR["Scott_emp",] <- c(mean(fdrScott_emp), mean(tprScott_emp), 0)
  }
  FDR.TPR["Storey",] <- c(mean(fdrStorey), mean(tprStorey), nrow(FDR.Storey)/nrow(nullHypSims)*100)
  FDR.TPR["BH",] <- c(mean(fdrBH), mean(tprBH), nrow(FDR.BH)/nrow(nullHypSims)*100)
  
  FDR.TPR
}

##------Function to run simulations for a specific alternative distribution with independent test statistics------##

run_sims_alt <- function(alt, nSims, pi0)
{
  if(alt %in% c("alt_beta"))
  {
    shape2 <- 20
    genPvalsInd <- genPvalsIndBeta
  }
  if(length(grep("_chisq_",alt))>0)
  {
    genPvalsInd <- genPvalsIndChisq
  }
  if(length(grep("_t_",alt))>0)
  {
    genPvalsInd <- genPvalsIndT
  }
  if(length(grep("_z_",alt))>0)
  {
    genPvalsInd <- genPvalsIndNorm
  }
  if(length(grep("_large",alt))>0)
  {
    shape2 <- 3
  }
  if(length(grep("_chisq_large",alt))>0)
  {
    shape2 <- 3
  }
  if(length(grep("_small",alt))>0)
  {
    shape2 <- 1
  }
  
  ##Simulate data
  cl<-makeCluster(8) ##specify number of cores less than or equal to number of cores on your computer
  registerDoParallel(cl)
  
  set.seed(1345)
  
  pValuesSims <- foreach(sim=1:nSims, .combine="rbind") %dorng% {

    if(length(grep("_3_3",alt))>0)
    {
      g <- genPvalsInd(pi0, shape2, 3, 3)
    } else {
      g <- genPvalsInd(pi0, shape2)
    }
    
    c(g$pValues, g$null, g$zValues)
  }
  
  ##close the cluster
  stopCluster(cl)
  
  pValuesSims
}

##------Function to run simulations for a specific alternative distribution with correlated test statistics------##

run_sims_alt_corr <- function(alt, nSims, pi0)
{
  if(length(grep("_t_",alt))>0)
  {
    genPvalsCorr <- genPvalsCorrT
  }
  if(length(grep("_z_",alt))>0)
  {
    genPvalsCorr <- genPvalsCorrNorm
  }
  if(length(grep("_large",alt))>0)
  {
    shape2 <- 3
  }
  if(length(grep("_small",alt))>0)
  {
    shape2 <- 1
  }
  ##get number of blocks
  nrBlocks <- as.numeric(as.character(strsplit(alt,"_")[[1]][4]))
  ##get within-block correlation
  rho <- as.numeric(as.character(strsplit(alt,"_")[[1]][5]))
  ##get the size of the block
  ntest <- length(pi0)
  sizeBlock <- ntest/nrBlocks
  
  ##make the block-diagonal matrices
  block <- matrix(rho, sizeBlock, sizeBlock)
  diag(block) <- 1
  blockList <- list()
  for(i in 1:nrBlocks)
  {
    blockList[[i]] <- block
  }
  Sigma <- bdiag(blockList)
  Sigma <- as.matrix(Sigma)
  
  ##Simulate data
  # cl<-makeCluster(8) ##specify number of cores less than or equal to number of cores on your computer
  # registerDoParallel(cl)
  
  set.seed(1345)

  pValuesSims <- matrix(NA, nrow=nSims, ncol=3*ntest)

  for(sim in 1:nSims) {
    
    g <- genPvalsCorr(pi0, shape2, Sigma)
    
    pValuesSims[sim,] <- c(g$pValues, g$null, g$zValues)
  }
      
  # pValuesSims <- foreach(sim=1:nSims, .combine="rbind", .packages="mnormt") %dorng% {
  # 
  #   g <- genPvalsCorr(pi0, shape2, Sigma)
  #   
  #   c(g$pValues, g$null, g$zValues)
  # }
  # 
  ##close the cluster
  # stopCluster(cl)
  
  pValuesSims
}


##------Function to run our method for a set of simulations------##

# estimate_pi0x_sims <- function(pValuesSims, X)
# {
#   nSims <- nrow(pValuesSims)
#   ntest <- ncol(pValuesSims)
#   
#   ##sequence of lambdas
#   lambdas <- round(seq(0.05, 0.95, 0.05),2)
#   which.0.8 <- which(lambdas==0.8)
#   which.0.9 <- which(lambdas==0.9)
#   
#   cl<-makeCluster(8) ##specify number of cores less than or equal to number of cores on your computer
#   registerDoParallel(cl)
#   
#   pi0EstSim <- foreach(sim = 1:nSims, .packages=c("swfdr")) %dorng% {  
#     res <- lm_pi0(pValuesSims[sim,], lambda=lambdas, X=X, 
#                   smooth.df=3, threshold=TRUE);
#     res.pi0.lambda <- res$pi0.lambda;
#     list(res.pi0.lambda[,which.0.8], 
#          res.pi0.lambda[,which.0.9],
#          res$pi0)}
#   
#   ##close the cluster
#   stopCluster(cl)
#   
#   pi0EstSim
# }

##do this without the lm_pi0 function in swfdr
estimate_pi0x_sims <- function(pValuesSims, X)
{
  nSims <- nrow(pValuesSims)
  ntest <- ncol(pValuesSims)
  
  ##sequence of lambdas
  lambdas <- round(seq(0.05, 0.95, 0.05),2)
  which.0.8 <- which(lambdas==0.8)
  which.0.9 <- which(lambdas==0.9)
  
  ##logistic regression version of estimate!
  lm_pi0 <- function(pValues, lambda = seq(0.05, 0.95, 0.05), X, smooth.df=3, threshold=TRUE)
  {
    ##if X is a vector, change it into a matrix
    if(is.null(dim(X)))
    {
      X <- matrix(X, ncol=1)
    }
    
    ##number of tests
    n <- nrow(X)
    ##number of lambdas
    nLambda <- length(lambda)
    
    ##sort lambdas from smallest to largest and take only unique values
    lambda <- sort(unique(lambda))
    
    ##make a design matrix with the intercept
    Xint <- cbind(1, X)
    
    ##get the estimate for each value of lambda 
    pi0.lambda <- matrix(NA, nrow=n, ncol=nLambda)
    for(i in 1:nLambda)
    {
      lambda.i <- lambda[i]
      y <- pValues > lambda.i
      
      ##fit regression
      regFit <- glm(y ~ X, family=binomial)
      
      ##get the estimated values of pi0
      pi0.lambda[,i] <- regFit$fitted.values/(1-lambda.i)
      
      if(threshold){
        pi0.lambda[,i] <- ifelse(pi0.lambda[,i] > 1, 1, pi0.lambda[,i])
        pi0.lambda[,i] <- ifelse(pi0.lambda[,i] < 0, 0, pi0.lambda[,i])
      }
    }
    
    ##smooth over values of lambda (do this for each test in part)
    pi0.smooth <- matrix(NA, nrow=n, ncol=nLambda)
    ##also save final estimate (maximum of 0 and minimum of 1 and smoothed value at largest lambda)
    pi0 <- rep(NA, length=n)
    for(i in 1:n)
    {
      if(i %% 10000==0)
      {
        message(paste("At test #:",i))
      }
      spi0 <- smooth.spline(lambda, pi0.lambda[i,], df=smooth.df)
      pi0.smooth[i, ] <- spi0$y
      pi0[i] <- pi0.smooth[i,nLambda]
    }
    
    if(threshold){ 
      pi0 <- ifelse(pi0 > 1, 1, pi0)
      pi0 <- ifelse(pi0 < 0, 0, pi0)
    }
    
    return(list(pi0=pi0, pi0.lambda=pi0.lambda, lambda=lambda, pi0.smooth=pi0.smooth))
  }
  
  cl<-makeCluster(8) ##specify number of cores less than or equal to number of cores on your computer
  registerDoParallel(cl)
  
  pi0EstSim <- foreach(sim = 1:nSims) %dorng% {  
    res <- lm_pi0(pValuesSims[sim,], lambda=lambdas, X=X, 
                  smooth.df=3);
    res.pi0.lambda <- res$pi0.lambda;
    list(res.pi0.lambda[,which.0.8], 
         res.pi0.lambda[,which.0.9],
         res$pi0)}
  
  ##close the cluster
  stopCluster(cl)
  
  pi0EstSim
}

##------Function to run Scott method for a set of simulations------##

estimate_Scott_sims <- function(zValuesSims, X, nulltype)
{
  nSims <- nrow(zValuesSims)
  ntest <- ncol(zValuesSims)
  
  cl<-makeCluster(8) ##specify number of cores less than or equal to number of cores on your computer
  registerDoParallel(cl)
  
  set.seed(31084)
  
  pi0hatScottMat <- foreach(sim=1:nSims, .combine="rbind", .packages="FDRreg", .errorhandling="pass") %dorng% {
    zScores <- zValuesSims[sim,]
    fdr <- FDRreg(zScores, X,
                  nulltype = nulltype,
                  control=list(lambda=1));
    if(length(fdr$priorprob) > 1)
    {
      pi0hatScott.sim <- c(1-fdr$priorprob, fdr$FDR);
    } else {
      pi0hatScott.sim <- rep(NA, 2*ntest);
    }
    pi0hatScott.sim
  }
  
  ##close the cluster
  stopCluster(cl)
  
  ##replace errors with NAs
  ##get rows with "M0 > 0 are not all TRUE"
  errorRows <- which(pi0hatScottMat[,1]=="M0 > 0 are not all TRUE")
  pi0hatScottMat[errorRows,] <- NA
  pi0hatScottMat <- apply(as.matrix(pi0hatScottMat), 2, as.numeric)
  
  pi0hatScottMat
}

##------Function to list the simulation results files------##

listSimRes <- function(alt, nr)
{
  c(paste(alt,"/simResults_", nr, ".RData",sep=""),
    paste(alt,"/simResults_pi0x_thresh_", nr, "_full.RData",sep=""),
    paste(alt,"/simResults_pi0x_Scott_", nr, "_full.RData",sep=""),
    paste(alt,"/simResults_pi0x_Scott_emp_", nr, "_full.RData",sep=""))
}

##------Function to get q-values for a set of simulations (have each simulation as a separate row)------##

getQValuesSimsBH <- function(pValuesSims)
{
  t(apply(pValuesSims, 1, p.adjust, method="BH"))
}
getQValuesSimsStorey <- function(pValuesSims)
{
  t(apply(pValuesSims, 1, function(p){t <- try(qvalue(p)$qvalues, silent=TRUE);
  if(mode(t)!="numeric"){t <- rep(NA, length=length(p))}; t}))
}

##------Function to get estimated FDR for our method for a set of simulations for the final, smoothed estimate (have each simulation as a separate row)------##

getFDRregSims <- function(pi0EstSim, qValuesSimsBH)
{
  ##first pull out just the final estimates
  pi0_final <- lapply(pi0EstSim, function(x){x[[3]]})
  t(mapply(function(q,pi0){q*pi0}, data.frame(t(qValuesSimsBH)), pi0_final, SIMPLIFY=TRUE))
}

```

```{r BL, warning = FALSE, message=FALSE}
# BiocManager::install("swfdr")
library(swfdr)

# lm_pi0()
```


# 5. AdaPT (Adaptive p-value thresholding)

```{r AdaPT, warning = FALSE, message=FALSE}
# BiocManager::install("adaptMT")

library(adaptMT)

# adapt_glm()
```

# 6. LFDR (local false discovery rate)

```{r LFDR, warning = FALSE, message=FALSE}
# install.packages("locfdr")

library(locfdr)

# locfdr()
```